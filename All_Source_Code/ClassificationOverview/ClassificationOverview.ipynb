{"nbformat": 4, "nbformat_minor": 2, "cells": [{"metadata": {}, "source": ["### Machine Learning for Engineers: [ClassificationOverview](https://www.apmonitor.com/pds/index.php/Main/ClassificationOverview)\n- [Classification with Machine Learning](https://www.apmonitor.com/pds/index.php/Main/ClassificationOverview)\n - Source Blocks: 5\n - Description: Supervised and unsupervised machine learning methods make a classification decision based on feature inputs.\n- [Course Overview](https://apmonitor.com/pds)\n- [Course Schedule](https://apmonitor.com/pds/index.php/Main/CourseSchedule)\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["# Create Support Vector Classifier\nclassifier = svm.SVC(gamma=0.001)\n# Learn / Fit\nclassifier.fit(X_train, y_train)\n# Predict\nclassifier.predict(digits.data[n:n+1])[0]"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["from sklearn import datasets, svm, metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# The digits dataset\ndigits = datasets.load_digits()\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# Split into train and test subsets (50% each)\nX_train, X_test, y_train, y_test = train_test_split(\n    data, digits.target, test_size=0.5, shuffle=False)\n\n# Learn the digits on the first half of the digits\nclassifier.fit(X_train, y_train)\n\n# test on second half of data\nn = np.random.randint(int(n_samples/2),n_samples)\nplt.imshow(digits.images[n], cmap=plt.cm.gray_r, interpolation='nearest')\nprint('Predicted: ' + str(classifier.predict(digits.data[n:n+1])[0]))\n\n# generate confusion matrix\ndisp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\nplt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["from sklearn import datasets, metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n\n# The digits dataset\ndigits = datasets.load_digits()\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Split into train and test subsets (50% each)\nXA, XB, yA, yB = train_test_split(\n    data, digits.target, test_size=0.5, shuffle=False)\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=2000)\n\n# Na\u00c3\u00afve Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss='modified_huber', shuffle=True,random_state=101,\\\n                    tol=1e-3,max_iter=1000)\n\n# K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=10,random_state=101,\\\n                               max_features=None,min_samples_leaf=5)\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators=70,oob_score=True,n_jobs=1,\\\n                  random_state=101,max_features=None,min_samples_leaf=3)\n\n# Support Vector Classifier\nfrom sklearn.svm import SVC\nsvm = SVC(gamma='scale', C=1.0, random_state=101)\n\n# Neural Network\nfrom sklearn.neural_network import MLPClassifier\nnn = MLPClassifier(solver='lbfgs',alpha=1e-5,max_iter=200,\\\n                    activation='relu',hidden_layer_sizes=(10,30,10),\\\n                    random_state=1, shuffle=True)\n\n# classification methods\nm = [nb,lr,sgd,knn,dtree,rfm,svm,nn]\ns = ['nb','lr','sgd','knn','dt','rfm','svm','nn']\n\n# fit classifiers\nprint('Train Classifiers')\nfor i,x in enumerate(m):\n    st = time.time()\n    x.fit(XA,yA)\n    tf = str(round(time.time()-st,5))\n    print(s[i] + ' time: ' + tf)\n\n# test on random number in second half of data\nn = np.random.randint(int(n_samples/2),n_samples)\nXt = digits.data[n:n+1]\n\n# test classifiers\nprint('Test Classifiers')\nfor i,x in enumerate(m):\n    st = time.time()\n    yt = x.predict(Xt)\n    tf = str(round(time.time()-st,5))\n    print(s[i] + ' predicts: ' + str(yt[0]) + ' time: ' + tf)\nprint('Label: ' + str(digits.target[n:n+1][0]))\n\nplt.imshow(digits.images[n], cmap=plt.cm.gray_r, interpolation='nearest')\nplt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["select_option = 3\n\ndata_options = ['linear','quadratic','target','moons','circles','blobs']\noption = data_options[select_option]\nn = 2000 # number of data points\n\nX = np.random.random((n,2))\nmixing = 0.0 # add random mixing element to data\nxplot = np.linspace(0,1,100)\n\nif option=='linear':\n    y = np.array([False if (X[i,0]+X[i,1])>=(1.0+mixing/2-np.random.rand()*mixing) \\\n                    else True \\\n                  for i in range(n)])\n    yplot = 1-xplot\nelif option=='quadratic':\n    y = np.array([False if X[i,0]**2>=X[i,1]+(np.random.rand()-0.5)*mixing \\\n                    else True \\\n                  for i in range(n)])\n    yplot = xplot**2\nelif option=='target':\n    y = np.array([False if (X[i,0]-0.5)**2+(X[i,1]-0.5)**2<=0.1 +(np.random.rand()-0.5)*0.2*mixing \\\n                    else True \\\n                  for i in range(n)])\n    j = False\n    yplot = np.empty(100)\n    for i,x in enumerate(xplot):\n        r = 0.1-(x-0.5)**2\n        if r<=0:\n            yplot[i] = np.nan\n        else:\n            j = not j # plot both sides of circle\n            yplot[i] = (2*j-1)*np.sqrt(r)+0.5\nelif option=='moons':\n    X, y = datasets.make_moons(n_samples=n,noise=0.05)\n    yplot = xplot*0.0\nelif option=='circles':\n    X, y = datasets.make_circles(n_samples=n,noise=0.05,factor=0.5)\n    yplot = xplot*0.0\nelif option=='blobs':\n    X, y = datasets.make_blobs(n_samples=n,centers=[[-5,3],[5,-3]],cluster_std=2.0)\n    yplot = xplot*0.0\n\nplt.scatter(X[y>0.5,0],X[y>0.5,1],color='blue',marker='^',label='True')\nplt.scatter(X[y<0.5,0],X[y<0.5,1],color='red',marker='x',label='False')\nif option not in ['moons','circles','blobs']:\n    plt.plot(xplot,yplot,'k.',label='Division')\nplt.legend()\n\n# Split into train and test subsets (50% each)\nXA, XB, yA, yB = train_test_split(X, y, test_size=0.5, shuffle=False)\n\n# Plot regression results\ndef assess(P):\n    plt.figure()\n    plt.scatter(XB[P==1,0],XB[P==1,1],marker='^',color='blue',label='True')\n    plt.scatter(XB[P==0,0],XB[P==0,1],marker='x',color='red',label='False')\n    plt.scatter(XB[P!=yB,0],XB[P!=yB,1],marker='s',color='orange',alpha=0.5,label='Incorrect')\n    if option not in ['moons','circles','blobs']:\n        plt.plot(xplot,yplot,'k.',label='Division')\n    plt.legend()"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["from sklearn import datasets, svm, metrics\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# The digits dataset\ndigits = datasets.load_digits()\n\n# Flatten the image to apply classifier\nn_samples = len(digits.images)\ndata = digits.images.reshape((n_samples, -1))\n\n# Create support vector classifier\nclassifier = svm.SVC(gamma=0.001)\n\n# Split into train and test subsets (50% each)\nX_train, X_test, y_train, y_test = train_test_split(\n    data, digits.target, test_size=0.5, shuffle=False)\n\n# Learn the digits on the first half of the digits\nclassifier.fit(X_train, y_train)\nn_samples/2\n\n# test on second half of data\nn = np.random.randint(int(n_samples/2),n_samples)\nplt.imshow(digits.images[n], cmap=plt.cm.gray_r, interpolation='nearest')\nprint('Predicted: ' + str(classifier.predict(digits.data[n:n+1])[0]))\n\n# Select Option by Number\n# 0 = Linear, 1 = Quadratic, 2 = Inner Target\n# 3 = Moons, 4 = Concentric Circles, 5 = Distinct Clusters\nselect_option = 5\n\n# generate data\ndata_options = ['linear','quadratic','target','moons','circles','blobs']\noption = data_options[select_option]\n# number of data points\nn = 2000\nX = np.random.random((n,2))\nmixing = 0.0 # add random mixing element to data\nxplot = np.linspace(0,1,100)\n\nif option=='linear':\n    y = np.array([False if (X[i,0]+X[i,1])>=(1.0+mixing/2-np.random.rand()*mixing) else True for i in range(n)])\n    yplot = 1-xplot\nelif option=='quadratic':\n    y = np.array([False if X[i,0]**2>=X[i,1]+(np.random.rand()-0.5)\\\n                  *mixing else True for i in range(n)])\n    yplot = xplot**2\nelif option=='target':\n    y = np.array([False if (X[i,0]-0.5)**2+(X[i,1]-0.5)**2<=0.1 +(np.random.rand()-0.5)*0.2*mixing else True for i in range(n)])\n    j = False\n    yplot = np.empty(100)\n    for i,x in enumerate(xplot):\n        r = 0.1-(x-0.5)**2\n        if r<=0:\n            yplot[i] = np.nan\n        else:\n            j = not j # plot both sides of circle\n            yplot[i] = (2*j-1)*np.sqrt(r)+0.5\nelif option=='moons':\n    X, y = datasets.make_moons(n_samples=n,noise=0.05)\n    yplot = xplot*0.0\nelif option=='circles':\n    X, y = datasets.make_circles(n_samples=n,noise=0.05,factor=0.5)\n    yplot = xplot*0.0\nelif option=='blobs':\n    X, y = datasets.make_blobs(n_samples=n,centers=[[-5,3],[5,-3]],cluster_std=2.0)\n    yplot = xplot*0.0\n\nplt.scatter(X[y>0.5,0],X[y>0.5,1],color='blue',marker='^',label='True')\nplt.scatter(X[y<0.5,0],X[y<0.5,1],color='red',marker='x',label='False')\nif option not in ['moons','circles','blobs']:\n    plt.plot(xplot,yplot,'k.',label='Division')\nplt.legend()\nplt.savefig(str(select_option)+'.png')\n\n# Split into train and test subsets (50% each)\nXA, XB, yA, yB = train_test_split(X, y, test_size=0.5, shuffle=False)\n\n# Plot regression results\ndef assess(P):\n    plt.figure()\n    plt.scatter(XB[P==1,0],XB[P==1,1],marker='^',color='blue',label='True')\n    plt.scatter(XB[P==0,0],XB[P==0,1],marker='x',color='red',label='False')\n    plt.scatter(XB[P!=yB,0],XB[P!=yB,1],marker='s',color='orange',alpha=0.5,label='Incorrect')\n    if option not in ['moons','circles','blobs']:\n        plt.plot(xplot,yplot,'k.',label='Division')\n    plt.legend()\n\n# Supervised Classification\n\n# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(XA,yA)\nyP = lr.predict(XB)\nassess(yP)\n\n# Na\u00c3\u00afve Bayes\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(XA,yA)\nyP = nb.predict(XB)\nassess(yP)\n\n# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss='modified_huber', shuffle=True,random_state=101)\nsgd.fit(XA,yA)\nyP = sgd.predict(XB)\nassess(yP)\n\n# K-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(XA,yA)\nyP = knn.predict(XB)\nassess(yP)\n\n# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=10,random_state=101,max_features=None,\\\n                       min_samples_leaf=5)\ndtree.fit(XA,yA)\nyP = dtree.predict(XB)\nassess(yP)\n\n# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators=70,oob_score=True,n_jobs=1,\\\n                  random_state=101,max_features=None,min_samples_leaf=3)\nrfm.fit(XA,yA)\nyP = rfm.predict(XB)\nassess(yP)\n\n# Support Vector Classifier\nfrom sklearn.svm import SVC\nsvm = SVC(gamma='scale', C=1.0, random_state=101)\nsvm.fit(XA,yA)\nyP = svm.predict(XB)\nassess(yP)\n\n# Neural Network\nfrom sklearn.neural_network import MLPClassifier\nclf = MLPClassifier(solver='lbfgs',alpha=1e-5,max_iter=200,\\\n                    activation='relu',hidden_layer_sizes=(10,30,10),\\\n                    random_state=1, shuffle=True)\nclf.fit(XA,yA)\nyP = clf.predict(XB)\nassess(yP)\n\n# Unsupervised Classification\n\n# K-Means Clustering\nfrom sklearn.cluster import KMeans\nkm = KMeans(n_clusters=2)\nkm.fit(XA)\nyP = km.predict(XB)\n# Arbitrary labels with unsupervised clustering may need to be reversed\nif len(XB[yP!=yB]) > n/4: yP = 1 - yP \nassess(yP)\n\n# Gaussian Mixture Model\nfrom sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=2)\ngmm.fit(XA)\nyP = gmm.predict_proba(XB) # produces probabilities\n# Arbitrary labels with unsupervised clustering may need to be reversed\nif len(XB[np.round(yP[:,0])!=yB]) > n/4: yP = 1 - yP \nassess(np.round(yP[:,0]))\n\n# Spectral Clustering\nfrom sklearn.cluster import SpectralClustering\nsc = SpectralClustering(n_clusters=2,eigen_solver='arpack',\\\n                        affinity='nearest_neighbors')\nyP = sc.fit_predict(XB) # No separation between fit and predict calls, need to fit and predict on same dataset\n# Arbitrary labels with unsupervised clustering may need to be reversed\nif len(XB[yP!=yB]) > n/4: yP = 1 - yP \nassess(yP)\n\nplt.show()"], "execution_count": null, "cell_type": "code"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}