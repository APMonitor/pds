{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for Engineers: [Classification Overview](https://www.apmonitor.com/pds/index.php/Main/ClassificationOverview)\n",
    "- [Classification with Machine Learning](https://www.apmonitor.com/pds/index.php/Main/ClassificationOverview)\n",
    " - Description: Supervised and unsupervised machine learning methods make a classification decision based on feature inputs.\n",
    "- [Course Overview](https://apmonitor.com/pds)\n",
    "- [Course Schedule](https://apmonitor.com/pds/index.php/Main/CourseSchedule)\n",
    "\n",
    "**Classify Images of Digits**\n",
    "\n",
    "<img align=\"left\" src=\"https://apmonitor.com/pds/uploads/Main/classify_numbers.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Create support vector classifier\n",
    "classifier = svm.SVC(gamma=0.001)\n",
    "\n",
    "# Split into train and test subsets (50% each)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Learn the digits on the first half of the digits\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# test on second half of data\n",
    "n = np.random.randint(int(n_samples/2),n_samples)\n",
    "plt.imshow(digits.images[n], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "print('Predicted: ' + str(classifier.predict(digits.data[n:n+1])[0]))\n",
    "\n",
    "# Get the predictions from the classifier\n",
    "predictions = classifier.predict(X_test)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions)\n",
    "\n",
    "# ConfusionMatrixDisplay to plot the confusion matrix\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lazypredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lazypredict.Supervised import LazyClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_digits()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                    test_size=.5,random_state=12)\n",
    "\n",
    "clf = LazyClassifier(verbose=0,ignore_warnings=True,custom_metric=None)\n",
    "models,predictions = clf.fit(X_train, X_test, y_train, y_test)\n",
    "\n",
    "print(models)\n",
    "models.to_csv('models.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# The digits dataset\n",
    "digits = datasets.load_digits()\n",
    "n_samples = len(digits.images)\n",
    "data = digits.images.reshape((n_samples, -1))\n",
    "\n",
    "# Split into train and test subsets (50% each)\n",
    "XA, XB, yA, yB = train_test_split(\n",
    "    data, digits.target, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs',multi_class='auto',max_iter=2000)\n",
    "\n",
    "# NaÃ¯ve Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='modified_huber', shuffle=True,random_state=101,\\\n",
    "                    tol=1e-3,max_iter=1000)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=10)\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(max_depth=10,random_state=101,\\\n",
    "                               max_features=None,min_samples_leaf=5)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfm = RandomForestClassifier(n_estimators=70,oob_score=True,n_jobs=1,\\\n",
    "                  random_state=101,max_features=None,min_samples_leaf=3)\n",
    "\n",
    "# Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='scale', C=1.0, random_state=101)\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "nn = MLPClassifier(solver='lbfgs',alpha=1e-5,max_iter=200,\\\n",
    "                    activation='relu',hidden_layer_sizes=(10,30,10),\\\n",
    "                    random_state=1, shuffle=True)\n",
    "\n",
    "# classification methods\n",
    "m = [nb,lr,sgd,knn,dtree,rfm,svm,nn]\n",
    "s = ['nb','lr','sgd','knn','dt','rfm','svm','nn']\n",
    "\n",
    "# fit classifiers\n",
    "print('Train Classifiers')\n",
    "for i,x in enumerate(m):\n",
    "    st = time.time()\n",
    "    x.fit(XA,yA)\n",
    "    tf = str(round(time.time()-st,5))\n",
    "    print(s[i] + ' time: ' + tf)\n",
    "\n",
    "# test on random number in second half of data\n",
    "n = np.random.randint(int(n_samples/2),n_samples)\n",
    "Xt = digits.data[n:n+1]\n",
    "\n",
    "# test classifiers\n",
    "print('Test Classifiers')\n",
    "for i,x in enumerate(m):\n",
    "    st = time.time()\n",
    "    yt = x.predict(Xt)\n",
    "    tf = str(round(time.time()-st,5))\n",
    "    print(s[i] + ' predicts: ' + str(yt[0]) + ' time: ' + tf)\n",
    "print('Label: ' + str(digits.target[n:n+1][0]))\n",
    "\n",
    "plt.imshow(digits.images[n], cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_option = 3\n",
    "\n",
    "data_options = ['linear','quadratic','target','moons','circles','blobs']\n",
    "option = data_options[select_option]\n",
    "n = 2000 # number of data points\n",
    "\n",
    "X = np.random.random((n,2))\n",
    "mixing = 0.0 # add random mixing element to data\n",
    "xplot = np.linspace(0,1,100)\n",
    "\n",
    "if option=='linear':\n",
    "    y = np.array([False if (X[i,0]+X[i,1])>=(1.0+mixing/2-np.random.rand()*mixing) \\\n",
    "                    else True \\\n",
    "                  for i in range(n)])\n",
    "    yplot = 1-xplot\n",
    "elif option=='quadratic':\n",
    "    y = np.array([False if X[i,0]**2>=X[i,1]+(np.random.rand()-0.5)*mixing \\\n",
    "                    else True \\\n",
    "                  for i in range(n)])\n",
    "    yplot = xplot**2\n",
    "elif option=='target':\n",
    "    y = np.array([False if (X[i,0]-0.5)**2+(X[i,1]-0.5)**2<=0.1 +(np.random.rand()-0.5)*0.2*mixing \\\n",
    "                    else True \\\n",
    "                  for i in range(n)])\n",
    "    j = False\n",
    "    yplot = np.empty(100)\n",
    "    for i,x in enumerate(xplot):\n",
    "        r = 0.1-(x-0.5)**2\n",
    "        if r<=0:\n",
    "            yplot[i] = np.nan\n",
    "        else:\n",
    "            j = not j # plot both sides of circle\n",
    "            yplot[i] = (2*j-1)*np.sqrt(r)+0.5\n",
    "elif option=='moons':\n",
    "    X, y = datasets.make_moons(n_samples=n,noise=0.05)\n",
    "    yplot = xplot*0.0\n",
    "elif option=='circles':\n",
    "    X, y = datasets.make_circles(n_samples=n,noise=0.05,factor=0.5)\n",
    "    yplot = xplot*0.0\n",
    "elif option=='blobs':\n",
    "    X, y = datasets.make_blobs(n_samples=n,centers=[[-5,3],[5,-3]],cluster_std=2.0)\n",
    "    yplot = xplot*0.0\n",
    "\n",
    "plt.scatter(X[y>0.5,0],X[y>0.5,1],color='blue',marker='^',label='True')\n",
    "plt.scatter(X[y<0.5,0],X[y<0.5,1],color='red',marker='x',label='False')\n",
    "if option not in ['moons','circles','blobs']:\n",
    "    plt.plot(xplot,yplot,'k.',label='Division')\n",
    "plt.legend()\n",
    "\n",
    "# Split into train and test subsets (50% each)\n",
    "XA, XB, yA, yB = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Plot regression results\n",
    "def assess(P):\n",
    "    plt.figure()\n",
    "    plt.scatter(XB[P==1,0],XB[P==1,1],marker='^',color='blue',label='True')\n",
    "    plt.scatter(XB[P==0,0],XB[P==0,1],marker='x',color='red',label='False')\n",
    "    plt.scatter(XB[P!=yB,0],XB[P!=yB,1],marker='s',color='orange',alpha=0.5,label='Incorrect')\n",
    "    if option not in ['moons','circles','blobs']:\n",
    "        plt.plot(xplot,yplot,'k.',label='Division')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Option by Number\n",
    "# 0 = Linear, 1 = Quadratic, 2 = Inner Target\n",
    "# 3 = Moons, 4 = Concentric Circles, 5 = Distinct Clusters\n",
    "select_option = 3\n",
    "\n",
    "# generate data\n",
    "data_options = ['linear','quadratic','target','moons','circles','blobs']\n",
    "option = data_options[select_option]\n",
    "# number of data points\n",
    "n = 2000\n",
    "X = np.random.random((n,2))\n",
    "mixing = 0.0 # add random mixing element to data\n",
    "xplot = np.linspace(0,1,100)\n",
    "\n",
    "if option=='linear':\n",
    "    y = np.array([False if (X[i,0]+X[i,1])>=(1.0+mixing/2-np.random.rand()*mixing) else True for i in range(n)])\n",
    "    yplot = 1-xplot\n",
    "elif option=='quadratic':\n",
    "    y = np.array([False if X[i,0]**2>=X[i,1]+(np.random.rand()-0.5)\\\n",
    "                  *mixing else True for i in range(n)])\n",
    "    yplot = xplot**2\n",
    "elif option=='target':\n",
    "    y = np.array([False if (X[i,0]-0.5)**2+(X[i,1]-0.5)**2<=0.1 +(np.random.rand()-0.5)*0.2*mixing else True for i in range(n)])\n",
    "    j = False\n",
    "    yplot = np.empty(100)\n",
    "    for i,x in enumerate(xplot):\n",
    "        r = 0.1-(x-0.5)**2\n",
    "        if r<=0:\n",
    "            yplot[i] = np.nan\n",
    "        else:\n",
    "            j = not j # plot both sides of circle\n",
    "            yplot[i] = (2*j-1)*np.sqrt(r)+0.5\n",
    "elif option=='moons':\n",
    "    X, y = datasets.make_moons(n_samples=n,noise=0.05)\n",
    "    yplot = xplot*0.0\n",
    "elif option=='circles':\n",
    "    X, y = datasets.make_circles(n_samples=n,noise=0.05,factor=0.5)\n",
    "    yplot = xplot*0.0\n",
    "elif option=='blobs':\n",
    "    X, y = datasets.make_blobs(n_samples=n,centers=[[-5,3],[5,-3]],cluster_std=2.0)\n",
    "    yplot = xplot*0.0\n",
    "\n",
    "plt.scatter(X[y>0.5,0],X[y>0.5,1],color='blue',marker='^',label='True')\n",
    "plt.scatter(X[y<0.5,0],X[y<0.5,1],color='red',marker='x',label='False')\n",
    "if option not in ['moons','circles','blobs']:\n",
    "    plt.plot(xplot,yplot,'k.',label='Division')\n",
    "plt.legend()\n",
    "plt.savefig(str(select_option)+'.png')\n",
    "\n",
    "# Split into train and test subsets (50% each)\n",
    "XA, XB, yA, yB = train_test_split(X, y, test_size=0.5, shuffle=False)\n",
    "\n",
    "# Plot regression results\n",
    "def assess(P):\n",
    "    plt.figure()\n",
    "    plt.scatter(XB[P==1,0],XB[P==1,1],marker='^',color='blue',label='True')\n",
    "    plt.scatter(XB[P==0,0],XB[P==0,1],marker='x',color='red',label='False')\n",
    "    plt.scatter(XB[P!=yB,0],XB[P!=yB,1],marker='s',color='orange',alpha=0.5,label='Incorrect')\n",
    "    if option not in ['moons','circles','blobs']:\n",
    "        plt.plot(xplot,yplot,'k.',label='Division')\n",
    "    plt.legend()\n",
    "\n",
    "# Supervised Classification\n",
    "\n",
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(XA,yA)\n",
    "yP = lr.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# NaÃ¯ve Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "nb = GaussianNB()\n",
    "nb.fit(XA,yA)\n",
    "yP = nb.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd = SGDClassifier(loss='modified_huber', shuffle=True,random_state=101)\n",
    "sgd.fit(XA,yA)\n",
    "yP = sgd.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# K-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(XA,yA)\n",
    "yP = knn.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier(max_depth=10,random_state=101,max_features=None,\\\n",
    "                       min_samples_leaf=5)\n",
    "dtree.fit(XA,yA)\n",
    "yP = dtree.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfm = RandomForestClassifier(n_estimators=70,oob_score=True,n_jobs=1,\\\n",
    "                  random_state=101,max_features=None,min_samples_leaf=3)\n",
    "rfm.fit(XA,yA)\n",
    "yP = rfm.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Support Vector Classifier\n",
    "from sklearn.svm import SVC\n",
    "svm = SVC(gamma='scale', C=1.0, random_state=101)\n",
    "svm.fit(XA,yA)\n",
    "yP = svm.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Neural Network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "clf = MLPClassifier(solver='lbfgs',alpha=1e-5,max_iter=200,\\\n",
    "                    activation='relu',hidden_layer_sizes=(10,30,10),\\\n",
    "                    random_state=1, shuffle=True)\n",
    "clf.fit(XA,yA)\n",
    "yP = clf.predict(XB)\n",
    "assess(yP)\n",
    "\n",
    "# Unsupervised Classification\n",
    "\n",
    "# K-Means Clustering\n",
    "try:\n",
    "    from sklearn.cluster import KMeans\n",
    "    km = KMeans(n_clusters=2)\n",
    "    km.fit(XA)\n",
    "    yP = km.predict(XB)\n",
    "    # Arbitrary labels with unsupervised clustering may need to be reversed\n",
    "    if len(XB[yP!=yB]) > n/4: yP = 1 - yP \n",
    "    assess(yP)\n",
    "except:\n",
    "    print('K-Means failed')\n",
    "    \n",
    "# Gaussian Mixture Model\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=2)\n",
    "gmm.fit(XA)\n",
    "yP = gmm.predict_proba(XB) # produces probabilities\n",
    "# Arbitrary labels with unsupervised clustering may need to be reversed\n",
    "if len(XB[np.round(yP[:,0])!=yB]) > n/4: yP = 1 - yP \n",
    "assess(np.round(yP[:,0]))\n",
    "\n",
    "# Spectral Clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "sc = SpectralClustering(n_clusters=2,eigen_solver='arpack',\\\n",
    "                        affinity='nearest_neighbors')\n",
    "yP = sc.fit_predict(XB) # No separation between fit and predict calls, \n",
    "                        #    need to fit and predict on same dataset\n",
    "# Arbitrary labels with unsupervised clustering may need to be reversed\n",
    "if len(XB[yP!=yB]) > n/4: yP = 1 - yP \n",
    "assess(yP)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
