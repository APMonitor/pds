{"nbformat": 4, "nbformat_minor": 2, "cells": [{"metadata": {}, "source": ["### Machine Learning for Engineers: [LinearRegression](https://www.apmonitor.com/pds/index.php/Main/LinearRegression)\n- [Linear Regression](https://www.apmonitor.com/pds/index.php/Main/LinearRegression)\n - Source Blocks: 4\n - Description: Perform univariate and multivariate linear regression with Python with and without parameter constraints.\n- [Course Overview](https://apmonitor.com/pds)\n- [Course Schedule](https://apmonitor.com/pds/index.php/Main/CourseSchedule)\n"], "cell_type": "markdown"}, {"metadata": {}, "outputs": [], "source": ["                            OLS Regression Results                            \n============================================================================\nDep. Variable:                      y  R-squared:                     0.897\nModel:                            OLS  Adj. R-squared:                0.880\nMethod:                 Least Squares  F-statistic:                   52.19\nDate:                Wed, 26 Aug 2020  Prob (F-statistic):         0.000357\nTime:                        22:05:45  Log-Likelihood:               2.9364\nNo. Observations:                   8  AIC:                          -1.873\nDf Residuals:                       6  BIC:                          -1.714\nDf Model:                           1                                        \nCovariance Type:          nonrobust                                         \n============================================================================\n             coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------\nx1         0.1980      0.027      7.224      0.000       0.131       0.265\nconst     -0.5432      0.115     -4.721      0.003      -0.825      -0.262\n============================================================================\nOmnibus:                    2.653   Durbin-Watson:                 0.811\nProb(Omnibus):              0.265   Jarque-Bera (JB):              0.918\nSkew:                       0.827   Prob(JB):                      0.632\nKurtosis:                   2.862   Cond. No.                       7.32\n============================================================================"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["import numpy as np\nfrom scipy.stats import linregress\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom gekko import GEKKO\n\n# Data\nx = np.array([4,5,2,3,-1,1,6,7])\ny = np.array([0.3,0.8,-0.05,0.1,-0.8,-0.5,0.5,0.65])\n\n# calculate R^2\ndef rsq(y1,y2):\n    yresid= y1 - y2\n    SSresid = np.sum(yresid**2)\n    SStotal = len(y1) * np.var(y1)\n    r2 = 1 - SSresid/SStotal\n    return r2\n\n# Method 1: scipy linregress\nslope,intercept,r,p_value,std_err = linregress(x,y)\na = [slope,intercept]\nprint('R^2 linregress = '+str(r**2))\n\n# Method 2: numpy polyfit (1=linear)\na = np.polyfit(x,y,1); print(a)\nyfit = np.polyval(a,x)\nprint('R^2 polyfit    = '+str(rsq(y,yfit)))\n\n# Method 3: numpy linalg solution\n#       y =     X a\n#   X^T y = X^T X a\nX = np.vstack((x,np.ones(len(x)))).T\n# matrix operations\nXX = np.dot(X.T,X)\nXTy = np.dot(X.T,y)\na = np.linalg.solve(XX,XTy)\n# same solution with lstsq\na = np.linalg.lstsq(X,y,rcond=None)[0]\nyfit = a[0]*x+a[1]; print(a)\nprint('R^2 matrix     = '+str(rsq(y,yfit)))\n\n# Method 4: statsmodels ordinary least squares\nX = sm.add_constant(x,prepend=False)\nmodel = sm.OLS(y,X).fit()\nyfit = model.predict(X)\na = model.params\nprint(model.summary())\n\n# Method 5: Gekko for constrained regression\nm = GEKKO(remote=False); m.options.IMODE=2\nc  = m.Array(m.FV,2); c[0].STATUS=1; c[1].STATUS=1\nc[1].lower=-0.5\nxd = m.Param(x); yd = m.Param(y); yp = m.Var()\nm.Equation(yp==c[0]*xd+c[1])\nm.Minimize((yd-yp)**2)\nm.solve(disp=False)\nc = [c[0].value[0],c[1].value[1]]\nprint(c)\n\n# plot data and regressed line\nplt.plot(x,y,'ko',label='data')\nxp = np.linspace(-2,8,100)\nslope     = str(np.round(a[0],2))\nintercept = str(np.round(a[1],2))\neqn = 'LstSQ: y='+slope+'x'+intercept\nplt.plot(xp,a[0]*xp+a[1],'r-',label=eqn)\nslope     = str(np.round(c[0],2))\nintercept = str(np.round(c[1],2))\neqn = 'Constraint: y='+slope+'x'+intercept\nplt.plot(xp,c[0]*xp+c[1],'b--',label=eqn)\nplt.grid()\nplt.legend()\nplt.show()"], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["                            OLS Regression Results                            \n==========================================================================\nDep. Variable:                      y   R-squared:                   0.933\nModel:                            OLS   Adj. R-squared:              0.906\nMethod:                 Least Squares   F-statistic:                 34.77\nDate:                Wed, 26 Aug 2020   Prob (F-statistic):        0.00117\nTime:                        23:16:24   Log-Likelihood:             4.6561\nNo. Observations:                   8   AIC:                        -3.312\nDf Residuals:                       5   BIC:                        -3.074\nDf Model:                           2                                         \nCovariance Type:        nonrobust                                         \n==========================================================================\n             coef    std err          t      P>|t|      [0.025      0.975]\n--------------------------------------------------------------------------\nx1         0.2003      0.024      8.256      0.000       0.138       0.263\nx2        -0.0750      0.046     -1.639      0.162      -0.193       0.043\nconst     -0.2883      0.186     -1.551      0.182      -0.766       0.190\n==========================================================================\nOmnibus:                    1.262   Durbin-Watson:                   1.558\nProb(Omnibus):              0.532   Jarque-Bera (JB):                0.075\nSkew:                      -0.237   Prob(JB):                        0.963\nKurtosis:                   3.026   Cond. No.                         16.9\n=========================================================================="], "execution_count": null, "cell_type": "code"}, {"metadata": {}, "outputs": [], "source": ["import numpy as np\nimport statsmodels.api as sm\nfrom gekko import GEKKO\n\n# Data\nx0 = np.array([4,5,2,3,-1,1,6,7])\nx1 = np.array([3,2,3,4, 3,5,2,6])\ny = np.array([0.3,0.8,-0.05,0.1,-0.8,-0.5,0.5,0.65])\n\n# calculate R^2\ndef rsq(y1,y2):\n    yresid= y1 - y2\n    SSresid = np.sum(yresid**2)\n    SStotal = len(y1) * np.var(y1)\n    r2 = 1 - SSresid/SStotal\n    return r2\n\n# Method 1: numpy linalg solution\n#       Y =     X a\n#   X^T Y = X^T X a\nX = np.vstack((x0,x1,np.ones(len(x0)))).T\na = np.linalg.lstsq(X,y)[0]; print(a)\nyfit = a[0]*x0+a[1]*x1+a[2]\nprint('R^2 = '+str(rsq(y,yfit)))\n\n# Method 2: statsmodels ordinary least squares\nmodel = sm.OLS(y,X).fit()\npredictions = model.predict(X)\nprint(model.summary())\n\n# Method 3: gekko\nm = GEKKO(remote=False); m.options.IMODE=2\nc  = m.Array(m.FV,3)\nfor ci in c:\n    ci.STATUS=1\nxd = m.Array(m.Param,2); xd[0].value=x0; xd[1].value=x1\nyd = m.Param(y); yp = m.Var()\ns =  m.sum([c[i]*xd[i] for i in range(2)])\nm.Equation(yp==s+c[-1])\nm.Minimize((yd-yp)**2)\nm.solve(disp=False)\na = [c[i].value[0] for i in range(3)]\nprint(a)\n\n# plot data\nfrom mpl_toolkits import mplot3d\nfrom matplotlib import cm\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nax  = plt.axes(projection='3d')\nax.plot3D(x0,x1,y,'ko')\nx0t = np.arange(-1,7,0.25)\nx1t = np.arange(2,6,0.25)\nX0,X1 = np.meshgrid(x0t,x1t)\nYt = a[0]*X0+a[1]*X1+a[2]\nax.plot_surface(X0,X1,Yt,cmap=cm.coolwarm,alpha=0.5)\nplt.show()"], "execution_count": null, "cell_type": "code"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}}