{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning for Engineers: [Cascade Classifier](https://www.apmonitor.com/pds/index.php/Main/CascadeClassifier)\n",
    "- [Computer Vision with Cascade Classifier](https://www.apmonitor.com/pds/index.php/Main/CascadeClassifier)\n",
    " - Description: Computer vision is how computers automate tasks that mimic human response to visual information. Image features such as points, edges, or objects are used to identify an object in an image.\n",
    "- [Course Overview](https://apmonitor.com/pds)\n",
    "- [Course Schedule](https://apmonitor.com/pds/index.php/Main/CourseSchedule)\n",
    "\n",
    "<img align=left width=400px src='https://apmonitor.com/pds/uploads/Main/cascade_classifier.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Faces in an Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import urllib.request\n",
    "\n",
    "# download image as class.jpg\n",
    "url = 'http://apmonitor.com/pds/uploads/Main/students_walking.jpg'\n",
    "urllib.request.urlretrieve(url, 'class.jpg')\n",
    "\n",
    "# face Cascade Classifier\n",
    "face=cv2.data.haarcascades+\"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "def draw_faces(data, result_list):\n",
    "    for i in range(len(result_list)):\n",
    "        x1, y1, width, height = result_list[i]\n",
    "        x2, y2 = x1 + width, y1 + height\n",
    "        plt.subplot(1, len(result_list), i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(data[y1:y2, x1:x2])\n",
    "\n",
    "pixels = plt.imread('class.jpg')\n",
    "faceCascade = cv2.CascadeClassifier(face)\n",
    "\n",
    "gray = cv2.cvtColor(pixels, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(gray,scaleFactor=1.1,\n",
    "                                     minNeighbors=2,\\\n",
    "                                     minSize=(10, 10))\n",
    "\n",
    "# display only the faces\n",
    "draw_faces(pixels, faces)             \n",
    "\n",
    "# display identified faces on original image\n",
    "fig, ax = plt.subplots(); ax.imshow(pixels)\n",
    "for (x, y, w, h) in faces:\n",
    "    rect = patches.Rectangle((x, y), w, h, lw=2, \\\n",
    "                             alpha=0.5, edgecolor='r', \\\n",
    "                             facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect Eyes or Faces from a Webcam Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import cv2\n",
    "import urllib.request\n",
    "\n",
    "# Get camera and display video\n",
    "camera = cv2.VideoCapture(0)\n",
    "WindowName = 'Cascade Classifier'\n",
    "cv2.namedWindow(WindowName, cv2.WINDOW_AUTOSIZE)\n",
    "\n",
    "# Trained cascade classifiers\n",
    "eye =cv2.data.haarcascades + \"haarcascade_eye.xml\"\n",
    "face=cv2.data.haarcascades+\"haarcascade_frontalface_default.xml\"\n",
    "\n",
    "# Cascade Classifier\n",
    "cascade=cv2.CascadeClassifier(face)\n",
    "\n",
    "start = time.time()\n",
    "while time.time()-start<=20.0:\n",
    "    # read frame\n",
    "    ret, pixels = camera.read()         \n",
    "    # convert to gray scale and identify faces\n",
    "    gray = cv2.cvtColor(pixels, cv2.COLOR_BGR2GRAY)\n",
    "    detect = cascade.detectMultiScale(gray,scaleFactor=1.1,\n",
    "                                           minNeighbors=3,\\\n",
    "                                           minSize=(30, 30))\n",
    "    # display identified faces on original image\n",
    "    for (x, y, w, h) in detect:\n",
    "        cv2.rectangle(pixels,(x,y),(x+w,y+h),(255,255,0),3)\n",
    "    cv2.imshow(WindowName,pixels)        \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release camera and video file\n",
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare to Deep Learning (MediaPipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(max_num_faces=1,refine_landmarks=True,\n",
    "    min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            # If loading a video, use 'break' instead of 'continue'.\n",
    "            continue\n",
    "\n",
    "        # To improve performance, optionally mark the image as not writeable to\n",
    "        # pass by reference.\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        # Draw the face mesh annotations on the image.\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        if results.multi_face_landmarks:\n",
    "            for lm in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(image=image,landmark_list=lm,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_tesselation_style())\n",
    "                mp_drawing.draw_landmarks(image=image,landmark_list=lm,\n",
    "                    connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_contours_style())\n",
    "                mp_drawing.draw_landmarks(image=image,landmark_list=lm,\n",
    "                    connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles\n",
    "                    .get_default_face_mesh_iris_connections_style())\n",
    "        # Flip the image horizontally for a selfie-view display.\n",
    "        cv2.imshow('MediaPipe Face Mesh', cv2.flip(image, 1))\n",
    "        if cv2.waitKey(5) & 0xFF == ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
