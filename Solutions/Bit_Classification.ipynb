{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bit Classification with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning (DL) is a subset of Machine Learning that uses Neural Network inspired architecture to make predictions. Convolutional Neural Networks (CNN) are a type of DL model that is effective in learning patterns in 2-dimensional data such as images. Images of drill bit types are used to train a classifier to identify common drill bit types. See [Machine Learning for Engineers course website](https://apmonitor.com/pds) for the source code to [Image Classification: Bits and Cracks](https://apmonitor.com/pds/index.php/Main/BitClassification).\n",
    "\n",
    "<img align=left width=400px src='https://apmonitor.com/pds/uploads/Main/bit_classification.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the following Python modules. Use ```pip``` to install any missing packages. For example, if there is an error: ```ModuleNotFoundError: No module named 'cv2'```, add a new cell and run the following command in another cell: ```pip install opencv-python```. An online search *how to install python cv2* may be needed if the name of the install package isn't the same as the import name. See additional information on [how to install and manage Python packages](https://apmonitor.com/pds/index.php/Main/InstallPythonPackages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yF35Ku0Ms-O6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import cv2\n",
    "import re\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Photos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download photos from [Drill Bit Images (bit_photos.zip)](http://apmonitor.com/pds/uploads/Main/bit_photos.zip). Download and extract the images by either selecting the [link](http://apmonitor.com/pds/uploads/Main/bit_photos.zip) or running the code cell below to download and unzip the archive.\n",
    "\n",
    "<img align=left width=400px src='https://apmonitor.com/pds/uploads/Main/bit_types.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download bit_photos.zip\n",
    "file = 'bit_photos.zip'\n",
    "url = 'http://apmonitor.com/pds/uploads/Main/'+file\n",
    "urllib.request.urlretrieve(url, file)\n",
    "\n",
    "# extract archive and remove bit_photos.zip\n",
    "with zipfile.ZipFile(file, 'r') as zip_ref:\n",
    "    zip_ref.extractall('./')\n",
    "os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The archive contains two folders, a test folder and train folder with subdirectories corresponding to the possible drill bit types (PDC, Roller Cone, and Spoon). The images are found within each subdirectory. The tree structure of the folders is:\n",
    "\n",
    "```\n",
    "├───test\n",
    "│   ├───PDC\n",
    "│   ├───Roller Cone\n",
    "│   └───Spoon\n",
    "└───train\n",
    "    ├───PDC\n",
    "    ├───Roller Cone\n",
    "    └───Spoon\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the photos into the Python session. The first step is to process the images into a format that 1) makes the data readable to the model, and 2) provides more training material for the model to learn. For example, the ```train_processor``` variable scales the data so that it can be a feature (input) for the model, but also takes each images and augments it so that the model can learn from multiple variations of the same picture. It flips it horizontally, rotates it, and shifts it, and more to make sure the model learns from the shape of the bit rather than the orientation or size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23726,
     "status": "ok",
     "timestamp": 1618065873113,
     "user": {
      "displayName": "Peter Van Katwyk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhknOaA2vSO9txrYBB1U01ff4hmNe3wj5xrjP9Nzg=s64",
      "userId": "16032387266776692701"
     },
     "user_tz": 360
    },
    "id": "topbCwEWsr3q",
    "outputId": "a4116caa-a435-41f2-d2d8-71ca1e307d8c"
   },
   "outputs": [],
   "source": [
    "# Data processing\n",
    "train_processor = ImageDataGenerator(rescale = 1./255, \\\n",
    "    horizontal_flip = True, zoom_range = 0.2,          \\\n",
    "    rotation_range = 10, shear_range = 0.2,            \\\n",
    "    height_shift_range = 0.1, width_shift_range = 0.1)\n",
    "test_processor = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "# Load data\n",
    "train = train_processor.flow_from_directory('train',   \\\n",
    "    target_size = (256, 256), batch_size = 32,         \\\n",
    "    class_mode = 'categorical', shuffle = True)\n",
    "\n",
    "test = test_processor.flow_from_directory('test',      \\\n",
    "    target_size = (256 ,256), batch_size = 32,         \\\n",
    "    class_mode = 'categorical', shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to build the CNN model with options for building the model. This includes the number of convolutional layers, fully connected dense layers, the number of nodes in each layer, and the number of training epochs. For more information on these parameters and Convolutional Neural Networks in general, see [Computer Vision with Deep Learning](https://apmonitor.com/pds/index.php/Main/VisionDeepLearning).  Change these parameters and analyze the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3288116,
     "status": "ok",
     "timestamp": 1618026147600,
     "user": {
      "displayName": "Peter Van Katwyk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhknOaA2vSO9txrYBB1U01ff4hmNe3wj5xrjP9Nzg=s64",
      "userId": "16032387266776692701"
     },
     "user_tz": 360
    },
    "id": "9-jCm9Y3Ehbs",
    "outputId": "c99cfeb8-dd44-4da7-e0b7-e97ba1455875"
   },
   "outputs": [],
   "source": [
    "# choose model parameters\n",
    "num_conv_layers = 2\n",
    "num_dense_layers = 1\n",
    "layer_size = 64\n",
    "num_training_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate model variable\n",
    "model = Sequential()\n",
    "\n",
    "# begin adding properties to model variable\n",
    "# e.g. add a convolutional layer\n",
    "model.add(Conv2D(layer_size, (3, 3), input_shape=(256,256, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# add additional convolutional layers based on num_conv_layers\n",
    "for _ in range(num_conv_layers-1):\n",
    "    model.add(Conv2D(layer_size, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# reduce dimensionality\n",
    "model.add(Flatten())\n",
    "\n",
    "# add fully connected \"dense\" layers if specified\n",
    "for _ in range(num_dense_layers):\n",
    "    model.add(Dense(layer_size))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "# add output layer\n",
    "model.add(Dense(3))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the sequential model with all added properties\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'],\n",
    "                )\n",
    "\n",
    "# use the data already loaded previously to train/tune the model\n",
    "model.fit(train,\n",
    "            epochs=num_training_epochs,\n",
    "            validation_data = test)\n",
    "\n",
    "# save the trained model\n",
    "model.save(f'bits.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained and saved as an `h5` file. The last line of the printed output contains the accuracy for both the training and testing data.\n",
    "\n",
    "```\n",
    "Epoch 19/20\n",
    "2/2 [====] - 3s 2s/step - loss: 0.8785 - accuracy: 0.6202 - val_loss: 0.9091 - val_accuracy: 0.6000\n",
    "Epoch 20/20\n",
    "2/2 [====] - 3s 2s/step - loss: 0.9028 - accuracy: 0.5721 - val_loss: 0.8648 - val_accuracy: 0.6667\n",
    "```\n",
    "\n",
    "The ```val_accuracy``` is the accuracy on the test images that are not included in the training. Hyperparameter optimization can be used to improve the accuracy by adjusting the CNN architecture, training selections, or other parameters. The function ```make_prediction``` takes the file path to a drill bit photo as an input and produces a classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "executionInfo": {
     "elapsed": 3080,
     "status": "ok",
     "timestamp": 1617992171243,
     "user": {
      "displayName": "Peter Van Katwyk",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjhknOaA2vSO9txrYBB1U01ff4hmNe3wj5xrjP9Nzg=s64",
      "userId": "16032387266776692701"
     },
     "user_tz": 360
    },
    "id": "Chpl4ZrSPtHc",
    "outputId": "cb9bd334-7945-4a5d-d765-6abd8f0f0eec",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "btype = ['PDC', 'Roller Cone', 'Spoon'] # possible output values\n",
    "def make_prediction(image_fp):\n",
    "    im = cv2.imread(image_fp) # load image\n",
    "    plt.imshow(im)\n",
    "    img = image.load_img(image_fp, target_size = (256,256))\n",
    "    img = image.img_to_array(img)\n",
    "\n",
    "    image_array = img / 255. # scale the image\n",
    "    img_batch = np.expand_dims(image_array, axis = 0)\n",
    "    \n",
    "    predicted_value = btype[model.predict(img_batch).argmax()]\n",
    "    true_value = re.search(r'(PDC)|(Roller Cone)|(Spoon)', image_fp)[0]\n",
    "    \n",
    "    out = f\"\"\"Predicted Bit Type: {predicted_value}\n",
    "    True Bit Type: {true_value}\n",
    "    Correct?: {predicted_value == true_value}\"\"\"\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly select type (1-3) and image number (1-5)\n",
    "i = random.randint(0,2); j = random.randint(1,5)\n",
    "b = btype[i]; im = b.replace(' ','_').lower() + '_' + str(j) + '.jpg'\n",
    "test_image_filepath = r'./test/'+b+'/'+im\n",
    "print(make_prediction(test_image_filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation accuracy as well as individual testing shows that there are misclassifications.\n",
    "\n",
    "Here are a few things that can improve the accuracy for this application:  \n",
    "- <b>More photos!</b> This is the most important thing, Machine Learning typically requires many photos that are representative of what the classifiers see. At this point, there are not nearly enough photos for the model to learn each bit type.\n",
    "- <b>Background Clutter.</b> Most of the images in this set have the background removed. To train a classifier to identify bit types in the field, more photos with realistic backgrounds are needed. Synthetic backgrounds can also be added.\n",
    "- <b>Hyperparameter Optimization.</b> To make the best model, the best parameters must be selected to maximize the accuracy (hyperparameter optimization). Packages such as [Hyperopt](http://hyperopt.github.io/hyperopt/) can intelligently try different parameter combinations to increase the accuracy without more data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=left width=550px src='https://apmonitor.com/pds/uploads/Main/activity.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity**: Replace the photos in the bit classification case study to build a classifier to distinguish photos of concrete with a crack (Positive) or no crack (Negative).\n",
    "\n",
    "```python\n",
    "url=http://apmonitor.com/pds/uploads/Main/concrete_cracks.zip\n",
    "```\n",
    "\n",
    "<img width=400px align=left src='http://apmonitor.com/pds/uploads/Main/concrete_cracks.png'>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "authorship_tag": "ABX9TyNhoK7FQHGoNgSPJqWd8qt+",
   "collapsed_sections": [],
   "name": "Bit_Classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
