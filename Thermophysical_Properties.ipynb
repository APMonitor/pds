{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thermophysical Property Prediction\n",
    "\n",
    "Empirical regression has limitations, especially when predictions are requested outside of the training region. Physics-based information can overcome this limitation by including fundamental engineering knowledge such as constraints during the training process.\n",
    "\n",
    "![Thermo Properties](https://apmonitor.com/pds/uploads/Main/thermophysical_properties.png)\n",
    "\n",
    "\n",
    "__Background__: Parachor values are a factor in the prediction of several thermophysical properties such as surface tension and thermal conductivity. The parachor value ($P$) is used to predict surface tension with the difference between the density of saturated liquid $\\rho_L$ and saturated vapor $\\rho_V$ at the temperature of interest.\n",
    "\n",
    "$\\sigma = \\left(P\\left(\\rho_L-\\rho_V\\right)\\right)^4$\n",
    "\n",
    "Surface tension and thermal conductivity are two specific properties that need improved predictions. A more accurate predictor of the parachor value (P) is an important step to improve those thermophysical properties. Most deep learning methods use a limited set of activation functions (ReLU, sigmoid, tanh, and linear) and perform unconstrained regression to minimize a loss function. The purpose of this case study is to explore the addition of physics-based information in the fitting process. This may include the use of new types of activation functions or constraints on the adjustable weights. The data for this case study is from Gharagheizi, et. al. (2011) who explored deep learning (a multi-layered neural network) to improve parachor predictions for 277 compounds from 40 functional groups.\n",
    "\n",
    "- Name: The common chemical name\n",
    "- Formula: Chemical formula of the compound\n",
    "- CASN: Chemical Abstracts Service Registry Number\n",
    "- Family: Chemical family of the compound\n",
    "- Parachor: Estimate of parachor value\n",
    "- Grp1-Grp40: Number of functional groups in the compound\n",
    "\n",
    "__Objective__: Develop a prediction of the parachar from the chemical compound data set. Report the correlation coefficient (R2) for predicting Parachor in the test set. Randomly select values that split the data into a train (80%) and test (20%) set. Use Linear Regression and Neural Network (Deep Learning) with constraints. The solutions for regression without constraints or feature engineering are provided in this notebook. For the constrained cases, enforce a positive parachor contribution for each group. Discuss the performance of each on the train and test sets. Submit source code and a summary memo (max 2 pages) of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "[Chemical Compound Data Set](https://apmonitor.com/pds/uploads/Main/thermo.txt)\n",
    "\n",
    "```python\n",
    "url = 'https://apmonitor.com/pds/uploads/Main/thermo.txt'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://apmonitor.com/pds/uploads/Main/thermo.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression without Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# input\n",
    "d = np.array(data)[:,-40:]\n",
    "d = np.array(d,dtype=float)\n",
    "\n",
    "d_train = d[0:205]\n",
    "d_valid = d[205:]\n",
    "\n",
    "# measured output\n",
    "meas = np.array(data['Parachor'])\n",
    "meas_train = meas[0:205]\n",
    "meas_valid = meas[205:]\n",
    "\n",
    "# linear regression\n",
    "#  d * b = p\n",
    "#  (d^T * d) * b = (d^T * meas)\n",
    "#  A * b = rhs\n",
    "A = np.dot(d_train.T,d_train)\n",
    "rhs = np.dot(d_train.T,meas_train)\n",
    "# solve for\n",
    "#  b = inv(d^T*d)*d^T*p\n",
    "b = np.linalg.solve(A,rhs)\n",
    "\n",
    "# predicted output\n",
    "pred_train = np.dot(d_train,b)\n",
    "pred_valid = np.dot(d_valid,b)\n",
    "\n",
    "print('ms_abs train')\n",
    "print(np.sum(np.abs((meas_train-pred_train)/meas_train)/(len(meas_train))))\n",
    "print('ms_abs validate')\n",
    "print(np.sum(np.abs((meas_valid-pred_valid)/meas_valid)/(len(meas_valid))))\n",
    "\n",
    "# parity plot\n",
    "plt.loglog([80,2000],[80,2000],'k-')\n",
    "plt.loglog(meas_train,pred_train,'b.',label='Linear (Train)')\n",
    "plt.loglog(meas_valid,pred_valid,'r.',label='Linear (Validate)')\n",
    "plt.legend()\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network (Deep Learning) without Constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "import matplotlib.pyplot as plt  \n",
    "\n",
    "#################################################################\n",
    "### Import Data #################################################\n",
    "#################################################################\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "# input\n",
    "d = np.array(data)[:,-40:]\n",
    "d = np.array(d,dtype=float)\n",
    "x_train = d[0:205]\n",
    "x_valid = d[205:]\n",
    "\n",
    "# measured output\n",
    "meas = np.array(data['Parachor'])\n",
    "y_train = meas[0:205]\n",
    "y_valid = meas[205:]\n",
    "\n",
    "train = np.vstack((x_train.T,y_train)).T\n",
    "valid = np.vstack((x_valid.T,y_valid)).T\n",
    "\n",
    "#################################################################\n",
    "### Scale data ##################################################\n",
    "#################################################################\n",
    "\n",
    "# scale values to 0 to 1 for the ANN to work well\n",
    "s = MinMaxScaler(feature_range=(0,1))\n",
    "\n",
    "# scale training and test data\n",
    "sc_train = s.fit_transform(train)\n",
    "xs_train = sc_train[:,0:-1]\n",
    "ys_train = sc_train[:,-1]\n",
    "\n",
    "sc_valid = s.transform(valid)\n",
    "xs_valid = sc_valid[:,0:-1]\n",
    "ys_valid = sc_valid[:,-1]\n",
    "\n",
    "#################################################################\n",
    "### Train model #################################################\n",
    "#################################################################\n",
    "\n",
    "# create neural network model\n",
    "model = Sequential()\n",
    "model.add(Dense(40, input_dim=40, activation='linear'))\n",
    "model.add(Dense(40, activation='linear'))\n",
    "model.add(Dense(5, activation='tanh'))\n",
    "model.add(Dense(5, activation='linear'))\n",
    "model.add(Dense(1, activation='linear'))\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "# load training data\n",
    "X1 = xs_train\n",
    "Y1 = ys_train\n",
    "\n",
    "# train the model\n",
    "model.fit(X1,Y1,epochs=200,verbose=1,shuffle=True)\n",
    "\n",
    "#################################################################\n",
    "### Test model ##################################################\n",
    "#################################################################\n",
    "\n",
    "# load test data\n",
    "X2 = xs_valid\n",
    "Y2 = ys_valid\n",
    "\n",
    "# test the model\n",
    "mse_train = model.evaluate(X1,Y1, verbose=1)\n",
    "mse_valid = model.evaluate(X2,Y2, verbose=1)\n",
    "\n",
    "print('Mean Squared Error (Train): ', mse_train)\n",
    "print('Mean Squared Error (Valid): ', mse_valid)\n",
    "\n",
    "#################################################################\n",
    "### Predictions Outside Training Region #########################\n",
    "#################################################################\n",
    "\n",
    "# predict\n",
    "Y1P = model.predict(X1)\n",
    "Y2P = model.predict(X2)\n",
    "\n",
    "# unscale for plotting and analysis\n",
    "ymin = s.min_[-1]\n",
    "yrange = s.scale_[-1]\n",
    "\n",
    "Y1u = (Y1-ymin)/yrange\n",
    "Y1Pu = (Y1P-ymin)/yrange\n",
    "\n",
    "Y2u = (Y2-ymin)/yrange\n",
    "Y2Pu = (Y2P-ymin)/yrange\n",
    "\n",
    "sae1 = 0.0\n",
    "for i in range(len(Y1u)):\n",
    "    sae1 += np.abs(Y1u[i]-Y1Pu[i][0])/Y1u[i]\n",
    "sae1 = sae1 / len(Y1u)\n",
    "\n",
    "sae2 = 0.0\n",
    "for i in range(len(Y2u)):\n",
    "    sae2 += np.abs(Y2u[i]-Y2Pu[i][0])/Y2u[i]\n",
    "sae2 = sae2 / len(Y2u)\n",
    "\n",
    "# mean sum abs difference\n",
    "print('Mean sum abs diff - Training ' + str(sae1))\n",
    "print('Mean sum abs diff - Validate ' + str(sae2))\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(Y1u, Y1Pu, 'b.',label='train')\n",
    "plt.plot(Y2u, Y2Pu, 'r.',label='validate')\n",
    "plt.xlabel('Measured')\n",
    "plt.ylabel('Predicted')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
